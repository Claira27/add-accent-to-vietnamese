# ThÃªm Dáº¥u Tiáº¿ng Viá»‡t

Dá»± Ã¡n nÃ y sá»­ dá»¥ng cÃ¡c hÃ m xá»­ lÃ½ tá»« cuá»™c thi **AIVIVN Challenge 3: ThÃªm dáº¥u tiáº¿ng Viá»‡t**.

## ğŸ”¤ Chuáº©n hoÃ¡ vÃ  MÃ£ hÃ³a tiáº¿ng Viá»‡t

Äá»ƒ Ä‘áº£m báº£o Ä‘Ã¡nh giÃ¡ nháº¥t quÃ¡n, táº¥t cáº£ tá»« tiáº¿ng Viá»‡t cÃ³ dáº¥u Ä‘Æ°á»£c chuáº©n hÃ³a vá» dáº¡ng **VNI** vá»›i quy táº¯c sau:

### Chuyá»ƒn Ä‘á»•i kÃ½ tá»± Ä‘áº·c biá»‡t:

| KÃ½ tá»± cÃ³ dáº¥u | MÃ£ hoÃ¡  |
| ------------ | ------- |
| Ã¢ / Ã‚        | a6 / A6 |
| Äƒ / Ä‚        | a8 / A8 |
| Ä‘ / Ä        | d9 / D9 |
| Ãª / ÃŠ        | e6 / E6 |
| Ã´ / Ã”        | o6 / O6 |
| Æ¡ / Æ         | o7 / O7 |
| Æ° / Æ¯        | u7 / U7 |

### MÃ£ hoÃ¡ thanh Ä‘iá»‡u:

- **Sáº¯c**: 1
- **Huyá»n**: 2
- **Há»i**: 3
- **NgÃ£**: 4
- **Náº·ng**: 5

**LÆ°u Ã½:** Dáº¥u sáº½ Ä‘Æ°á»£c Ä‘áº©y vá» **cuá»‘i má»—i tá»«**.

---

## ğŸ“š Dá»¯ liá»‡u

- **Input vocab size**: 1,450 tá»« khÃ´ng dáº¥u
- **Output vocab size**: 5,805 tá»« cÃ³ dáº¥u
- **Padding token**: `<pad>` (chá»‰ sá»‘ **5802** trong vocab Ä‘áº§u ra)

### Dataset:

| Táº­p dá»¯ liá»‡u   | Sá»‘ lÆ°á»£ng cÃ¢u |
| ------------- | ------------ |
| train_dataset | 4,393,646    |
| val_dataset   | 549,205      |
| test_dataset  | 549,207      |

---

## ğŸ§  Kiáº¿n trÃºc mÃ´ hÃ¬nh & ÄÃ¡nh giÃ¡

### âœ… LSTM (Seq2Seq + Attention)

- **Embedding dim**: 256
- **Hidden dim**: 512
- **Num layers**: 1
- **Inference**: Beam Search (beam size = 3)
- **Sá»‘ tham sá»‘**: ~10 triá»‡u
- **Káº¿t quáº£**:
  - Accuracy > **90%** (sau 2 epoch)
  - BLEU score > **87%**
  - Hiá»‡u quáº£, á»•n Ä‘á»‹nh vÃ  dá»… huáº¥n luyá»‡n

---

### ğŸ§  Transformer

- **DIM_MODEL** = 512
- **N_HEADS** = 8
- **N_LAYERS** = 4
- **MAX_LEN** = 50
- **Inference**: Greedy decode
- **Sá»‘ tham sá»‘**: hÆ¡n 23 triá»‡u
- **Káº¿t quáº£**:
  - Accuracy > **90%** (sau 3 epoch)
  - BLEU score â‰ˆ **79.8%**
  - **NguyÃªn nhÃ¢n BLEU tháº¥p hÆ¡n**: Transformer cÃ³ xu hÆ°á»›ng nháº§m giá»¯a cÃ¡c thanh (dáº¥u há»i/ngÃ£) khi huáº¥n luyá»‡n chÆ°a Ä‘á»§ dÃ i hoáº·c dá»¯ liá»‡u khÃ´ng cÃ¢n báº±ng theo kiá»ƒu dáº¥u. MÃ´ hÃ¬nh chÆ°a há»™i tá»¥ hoÃ n toÃ n do tÃ i nguyÃªn huáº¥n luyá»‡n cÃ³ háº¡n.

---

## ğŸ“ Cáº¥u trÃºc thÆ° má»¥c

```plaintext
.
â”œâ”€â”€ test/
â”‚   â”œâ”€â”€ lstm_test.py             # Kiá»ƒm tra mÃ´ hÃ¬nh LSTM
â”‚   â””â”€â”€ transformer_test.py      # Kiá»ƒm tra mÃ´ hÃ¬nh Transformer
â”‚   â””â”€â”€ utils.py                 # HÃ m encode/decode tá»« sang ID vÃ  ngÆ°á»£c láº¡i
â”œâ”€â”€ train&results/
â”‚   â”œâ”€â”€ train_lstm               # Huáº¥n luyá»‡n mÃ´ hÃ¬nh LSTM, model vÃ  káº¿t quáº£
â”‚   â””â”€â”€ train_transformer        # Huáº¥n luyá»‡n mÃ´ hÃ¬nh Transformer, model vÃ  káº¿t quáº£
```

# Cháº¡y thá»­ vá»›i mÃ´ hÃ¬nh LSTM

python test/lstm_test.py

# Cháº¡y thá»­ vá»›i Transformer

python test/transformer_test.py

# Nháº­n xÃ©t
LSTM Ä‘áº¡t Ä‘á»™ chÃ­nh xÃ¡c 88% vá»›i phÆ°Æ¡ng phÃ¡p mÃ£ hÃ³a tiÃªu chuáº©n, cáº£i thiá»‡n hiá»‡u suáº¥t 10% so vá»›i phÆ°Æ¡ng phÃ¡p nhÃºng cÆ¡ báº£n.
LSTM vá»›i attention lÃ  lá»±a chá»n hiá»‡u quáº£ hÆ¡n cho bÃ i toÃ¡n thÃªm dáº¥u tiáº¿ng Viá»‡t vá»›i dá»¯ liá»‡u vÃ  tÃ i nguyÃªn háº¡n cháº¿.
Transformer cÃ³ tiá»m nÄƒng, nhÆ°ng yÃªu cáº§u huáº¥n luyá»‡n lÃ¢u hÆ¡n vÃ  tinh chá»‰nh ká»¹ hÆ¡n.
