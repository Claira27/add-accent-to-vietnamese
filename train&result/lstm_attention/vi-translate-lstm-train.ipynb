{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-05-31T11:07:18.018610Z",
     "iopub.status.busy": "2025-05-31T11:07:18.018041Z",
     "iopub.status.idle": "2025-05-31T11:07:28.017418Z",
     "shell.execute_reply": "2025-05-31T11:07:28.016393Z",
     "shell.execute_reply.started": "2025-05-31T11:07:18.018588Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!cp -r \"/kaggle/input/vi-tone-no-tone/data\" /kaggle/working/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-31T11:07:38.174750Z",
     "iopub.status.busy": "2025-05-31T11:07:38.173977Z",
     "iopub.status.idle": "2025-05-31T11:07:45.631605Z",
     "shell.execute_reply": "2025-05-31T11:07:45.631085Z",
     "shell.execute_reply.started": "2025-05-31T11:07:38.174720Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "import time\n",
    "import random\n",
    "import re\n",
    "from torch import amp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-31T11:07:52.994165Z",
     "iopub.status.busy": "2025-05-31T11:07:52.993480Z",
     "iopub.status.idle": "2025-05-31T11:08:17.240776Z",
     "shell.execute_reply": "2025-05-31T11:08:17.240097Z",
     "shell.execute_reply.started": "2025-05-31T11:07:52.994138Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences in train_dataset: 4393646\n",
      "Number of sentences in val_dataset: 549205\n",
      "Number of sentences in test_dataset: 549207\n",
      "Expected train batches: 34326\n",
      "Input vocab size: 1450\n",
      "Output vocab size: 5805\n",
      "Sample input vocab: [('a', 0), ('ac', 1), ('ach', 2), ('ai', 3), ('am', 4)]\n",
      "Sample output vocab: [('a', 0), ('a1', 1), ('a1c', 2), ('a1ch', 3), ('a1i', 4)]\n",
      "Value of <pad>: 5802\n",
      "Sample source: ['tenedos barronus uoc ralph vary chamberlin mieu ta nam', 'ngay giao su tran van huong uoc quoc truong phan khac suu bo nhiem lam thu tuong', 'trong noi inh cac vi quy nhan cung cung tan co ngau nhien lam sai ieu gi quach hau cung khong truy cuu con o truoc mat tao phi bao che', 'chung uoc su dung cho che tao cac cam bien tia hong ngoai hoac nhiet ien', 'uchukeiji gyaban uoc bat au tu mot tam hinh minh hoa cua murakami katsushi mot nhan vien thiet ke cua hang bandai nguoi a e lai ten tuoi minh trong lich su nganh o choi voi nhieu san pham oc ao']\n",
      "Sample target: ['tenedos barronus d9u7o75c ralph vary chamberlin mie6u ta3 na8m', 'nga2y gia1o su7 tra62n va8n hu7o7ng d9u7o75c quo61c tru7o73ng phan kha81c su73u bo63 nhie65m la2m thu3 tu7o71ng', 'trong no65i d9i2nh ca1c vi5 quy1 nha6n cu2ng cung ta62n co1 nga64u nhie6n la2m sai d9ie62u gi2 qua1ch ha65u cu4ng kho6ng truy cu71u co2n o73 tru7o71c ma85t ta2o phi bao che', 'chu1ng d9u7o75c su73 du5ng cho che61 ta5o ca1c ca3m bie61n tia ho62ng ngoa5i hoa85c nhie65t d9ie65n', 'uchukeiji gyaban d9u7o75c ba81t d9a62u tu72 mo65t ta61m hi2nh minh ho5a cu3a murakami katsushi mo65t nha6n vie6n thie61t ke61 cu3a ha4ng bandai ngu7o72i d9a4 d9e63 la5i te6n tuo63i mi2nh trong li5ch su73 nga2nh d9o62 cho7i vo71i nhie62u sa3n pha63m d9o65c d9a1o']\n"
     ]
    }
   ],
   "source": [
    "# Tạo thư mục lưu kết quả\n",
    "os.makedirs(\"/kaggle/working/results\", exist_ok=True)\n",
    "\n",
    "# Định nghĩa lớp Dataset\n",
    "class TranslationDatasetFull(Dataset):\n",
    "    def __init__(self, in_file, out_file, in_vocab, out_vocab, max_len=50):\n",
    "        self.in_sentences = self._load_sentences(in_file)\n",
    "        self.out_sentences = self._load_sentences(out_file)\n",
    "        self.in_vocab = self._load_vocab(in_vocab) if isinstance(in_vocab, str) else in_vocab\n",
    "        self.out_vocab = self._load_vocab(out_vocab) if isinstance(out_vocab, str) else out_vocab\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def _load_sentences(self, file_path):\n",
    "        df = pd.read_csv(file_path, encoding='utf-8', header=None, names=['ID', 'Sentence'])\n",
    "        sentences = df['Sentence'].tolist()\n",
    "        return [str(s).strip() for s in sentences if str(s).strip()]\n",
    "\n",
    "    def _load_vocab(self, vocab_path):\n",
    "        vocab = {}\n",
    "        with open(vocab_path, 'r', encoding='utf-8') as f:\n",
    "            words = [line.strip() for line in f if line.strip()]\n",
    "        for idx, word in enumerate(words):\n",
    "            vocab[word] = idx\n",
    "        required_tokens = ['<unk>', '<pad>', '<sos>', '<eos>']\n",
    "        max_idx = max(vocab.values()) if vocab else -1\n",
    "        for token in required_tokens:\n",
    "            if token not in vocab:\n",
    "                max_idx += 1\n",
    "                vocab[token] = max_idx\n",
    "        return vocab\n",
    "\n",
    "    def _encode_sentence(self, sentence, vocab, max_len):\n",
    "        tokens = sentence.strip().split()\n",
    "        token_ids = [vocab.get(token, vocab['<unk>']) for token in tokens]\n",
    "        token_ids = token_ids[:max_len] + [vocab['<pad>']] * (max_len - len(token_ids))\n",
    "        return token_ids\n",
    "\n",
    "    def _encode_decoder_sentence(self, sentence, vocab, max_len):\n",
    "        tokens = sentence.strip().split()\n",
    "        full_tokens = [vocab['<sos>']] + [vocab.get(token, vocab['<unk>']) for token in tokens] + [vocab['<eos>']]\n",
    "        if len(full_tokens) < max_len:\n",
    "            full_tokens += [vocab['<pad>']] * (max_len - len(full_tokens))\n",
    "        else:\n",
    "            full_tokens = full_tokens[:max_len]\n",
    "        return full_tokens\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.in_sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        in_sentence = self.in_sentences[idx]\n",
    "        out_sentence = self.out_sentences[idx]\n",
    "        src = self._encode_sentence(in_sentence, self.in_vocab, self.max_len)\n",
    "        tgt = self._encode_decoder_sentence(out_sentence, self.out_vocab, self.max_len)\n",
    "        return torch.tensor(src, dtype=torch.long), torch.tensor(tgt, dtype=torch.long)\n",
    "\n",
    "# Tạo dataset và DataLoader\n",
    "train_dataset = TranslationDatasetFull(\n",
    "    \"/kaggle/working/data/train/source.csv\",\n",
    "    \"/kaggle/working/data/train/target.csv\",\n",
    "    \"/kaggle/working/data/vocab/input_vocab.txt\",\n",
    "    \"/kaggle/working/data/vocab/output_vocab.txt\",\n",
    "    max_len=50\n",
    ")\n",
    "val_dataset = TranslationDatasetFull(\n",
    "    \"/kaggle/working/data/val/source.csv\",\n",
    "    \"/kaggle/working/data/val/target.csv\",\n",
    "    \"/kaggle/working/data/vocab/input_vocab.txt\",\n",
    "    \"/kaggle/working/data/vocab/output_vocab.txt\",\n",
    "    max_len=50\n",
    ")\n",
    "test_dataset = TranslationDatasetFull(\n",
    "    \"/kaggle/working/data/test/source.csv\",\n",
    "    \"/kaggle/working/data/test/target.csv\",\n",
    "    \"/kaggle/working/data/vocab/input_vocab.txt\",\n",
    "    \"/kaggle/working/data/vocab/output_vocab.txt\",\n",
    "    max_len=50\n",
    ")\n",
    "\n",
    "batch_size = 128\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True, num_workers=2)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, pin_memory=True, num_workers=2)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, pin_memory=True, num_workers=2)\n",
    "\n",
    "# Debug dataset và vocab\n",
    "print(f\"Number of sentences in train_dataset: {len(train_dataset)}\")\n",
    "print(f\"Number of sentences in val_dataset: {len(val_dataset)}\")\n",
    "print(f\"Number of sentences in test_dataset: {len(test_dataset)}\")\n",
    "print(f\"Expected train batches: {len(train_loader)}\")\n",
    "print(\"Input vocab size:\", len(train_dataset.in_vocab))\n",
    "print(\"Output vocab size:\", len(train_dataset.out_vocab))\n",
    "print(\"Sample input vocab:\", list(train_dataset.in_vocab.items())[:5])\n",
    "print(\"Sample output vocab:\", list(train_dataset.out_vocab.items())[:5])\n",
    "print(\"Value of <pad>:\", train_dataset.out_vocab.get('<pad>', \"Not found\"))\n",
    "print(\"Sample source:\", train_dataset.in_sentences[:5])\n",
    "print(\"Sample target:\", train_dataset.out_sentences[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-31T11:09:23.827107Z",
     "iopub.status.busy": "2025-05-31T11:09:23.826452Z",
     "iopub.status.idle": "2025-05-31T11:09:28.845690Z",
     "shell.execute_reply": "2025-05-31T11:09:28.844997Z",
     "shell.execute_reply.started": "2025-05-31T11:09:23.827080Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Định nghĩa mô hình LSTM\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class EncoderLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers, dropout=0.5):\n",
    "        super(EncoderLSTM, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=train_dataset.in_vocab['<pad>'])\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, batch_first=True, dropout=dropout if num_layers > 1 else 0)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src):\n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "        outputs, (hidden, cell) = self.lstm(embedded)\n",
    "        return outputs, hidden, cell\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        self.attn = nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "        self.v = nn.Parameter(torch.rand(hidden_dim))\n",
    "        stdv = 1. / (self.v.size(0) ** 0.5)\n",
    "        self.v.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        batch_size = encoder_outputs.shape[0]\n",
    "        src_len = encoder_outputs.shape[1]\n",
    "        hidden = hidden[-1].unsqueeze(1).repeat(1, src_len, 1)\n",
    "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=2)))\n",
    "        attention = torch.sum(self.v * energy, dim=2)\n",
    "        return F.softmax(attention, dim=1)\n",
    "\n",
    "class DecoderLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers, dropout=0.5):\n",
    "        super(DecoderLSTM, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=train_dataset.out_vocab['<pad>'])\n",
    "        self.lstm = nn.LSTM(embedding_dim + hidden_dim, hidden_dim, num_layers, batch_first=True, dropout=dropout if num_layers > 1 else 0)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.attention = Attention(hidden_dim)\n",
    "\n",
    "    def forward(self, tgt, hidden, cell, encoder_outputs):\n",
    "        embedded = self.dropout(self.embedding(tgt))\n",
    "        attn_weights = self.attention(hidden, encoder_outputs)\n",
    "        attn_weights = attn_weights.unsqueeze(1)\n",
    "        context = torch.bmm(attn_weights, encoder_outputs)\n",
    "        lstm_input = torch.cat((embedded, context), dim=2)\n",
    "        output, (hidden, cell) = self.lstm(lstm_input, (hidden, cell))\n",
    "        prediction = self.fc(output.squeeze(1))\n",
    "        return prediction, hidden, cell, attn_weights\n",
    "\n",
    "class Seq2SeqLSTM(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super(Seq2SeqLSTM, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, src, tgt, teacher_forcing_ratio=0.8):\n",
    "        batch_size = src.shape[0]\n",
    "        tgt_len = tgt.shape[1]\n",
    "        tgt_vocab_size = self.decoder.fc.out_features\n",
    "        outputs = torch.zeros(batch_size, tgt_len, tgt_vocab_size).to(self.device)\n",
    "        encoder_outputs, hidden, cell = self.encoder(src)\n",
    "        input = tgt[:, 0]\n",
    "        for t in range(1, tgt_len):\n",
    "            output, hidden, cell, _ = self.decoder(input.unsqueeze(1), hidden, cell, encoder_outputs)\n",
    "            outputs[:, t, :] = output\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            top1 = output.argmax(1)\n",
    "            input = tgt[:, t] if teacher_force else top1\n",
    "        return outputs\n",
    "\n",
    "# Khởi tạo mô hình\n",
    "embedding_dim = 256\n",
    "hidden_dim = 512\n",
    "num_layers = 1\n",
    "dropout = 0.5\n",
    "\n",
    "input_vocab_size = len(train_dataset.in_vocab)\n",
    "output_vocab_size = len(train_dataset.out_vocab)\n",
    "\n",
    "encoder = EncoderLSTM(input_vocab_size, embedding_dim, hidden_dim, num_layers, dropout).to(device)\n",
    "decoder = DecoderLSTM(output_vocab_size, embedding_dim, hidden_dim, num_layers, dropout).to(device)\n",
    "model = Seq2SeqLSTM(encoder, decoder, device).to(device)\n",
    "\n",
    "# Định nghĩa loss, optimizer và scaler\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=train_dataset.out_vocab['<pad>'])\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, verbose=True)\n",
    "scaler = torch.amp.GradScaler('cuda')\n",
    "\n",
    "# Hàm beam search (giữ đầu ra mã hóa)\n",
    "def beam_search(model, src, in_vocab, out_vocab, max_len, device, beam_size=3):\n",
    "    model.eval()\n",
    "    src = src.to(device)\n",
    "    start_token = out_vocab['<sos>']\n",
    "    end_token = out_vocab['<eos>']\n",
    "    with torch.no_grad():\n",
    "        encoder_outputs, hidden, cell = model.encoder(src)\n",
    "    sequences = [[torch.tensor([start_token], dtype=torch.long, device=device), 0.0, hidden, cell]]\n",
    "    completed_sequences = []\n",
    "    for _ in range(max_len):\n",
    "        all_candidates = []\n",
    "        for seq, score, hidden, cell in sequences:\n",
    "            if seq[-1].item() == end_token:\n",
    "                completed_sequences.append([seq, score])\n",
    "                continue\n",
    "            with torch.no_grad():\n",
    "                output, hidden, cell, _ = model.decoder(seq[-1].unsqueeze(0).unsqueeze(1), hidden, cell, encoder_outputs)\n",
    "            probs = F.softmax(output.squeeze(1), dim=1)\n",
    "            top_probs, top_indices = probs.topk(beam_size, dim=1)\n",
    "            for i in range(beam_size):\n",
    "                token = top_indices[0, i].unsqueeze(0)\n",
    "                token_prob = top_probs[0, i].item()\n",
    "                new_seq = torch.cat((seq, token), dim=0)\n",
    "                new_score = score + torch.log(torch.tensor(token_prob))\n",
    "                all_candidates.append([new_seq, new_score, hidden, cell])\n",
    "        all_candidates = sorted(all_candidates, key=lambda x: x[1], reverse=True)\n",
    "        sequences = all_candidates[:beam_size]\n",
    "        if len(sequences) == 0:\n",
    "            break\n",
    "    if completed_sequences:\n",
    "        best_seq = max(completed_sequences, key=lambda x: x[1])[0]\n",
    "    else:\n",
    "        best_seq = sequences[0][0]\n",
    "    idx2word = {idx: word for word, idx in out_vocab.items()}\n",
    "    translated = [idx2word.get(token.item(), '<unk>') for token in best_seq[1:] if token.item() != end_token]\n",
    "    return translated\n",
    "\n",
    "# Hàm suy luận với câu đầu vào\n",
    "def predict_sentence(model, sentence, in_vocab, out_vocab, max_len, device, beam_size=3):\n",
    "    model.eval()\n",
    "    tokens = sentence.strip().split()\n",
    "    token_ids = [in_vocab.get(token, in_vocab['<unk>']) for token in tokens]\n",
    "    token_ids = token_ids[:max_len] + [in_vocab['<pad>']] * (max_len - len(token_ids))\n",
    "    src = torch.tensor([token_ids], dtype=torch.long).to(device)\n",
    "    pred_tokens = beam_search(model, src, in_vocab, out_vocab, max_len, device, beam_size)\n",
    "    normalized_sentence = ' '.join(pred_tokens)\n",
    "    return normalized_sentence\n",
    "\n",
    "# Hàm tính metrics với beam search\n",
    "def calculate_metrics(model, iterator, in_vocab, out_vocab, device, max_len, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    total_tokens = 0\n",
    "    bleu_scores = []\n",
    "    idx2word = {idx: word for word, idx in out_vocab.items()}\n",
    "    smoothie = SmoothingFunction().method4\n",
    "    total_samples = len(iterator.dataset)\n",
    "    sample_size = max(1, total_samples // 100)\n",
    "    sampled_indices = random.sample(range(total_samples), sample_size)\n",
    "    sampled_dataset = torch.utils.data.Subset(iterator.dataset, sampled_indices)\n",
    "    sampled_loader = DataLoader(sampled_dataset, batch_size=iterator.batch_size, shuffle=False, pin_memory=True, num_workers=4)\n",
    "    with torch.no_grad():\n",
    "        for src, tgt in iterator:\n",
    "            src, tgt = src.to(device), tgt.to(device)\n",
    "            output = model(src, tgt, teacher_forcing_ratio=0)\n",
    "            output_dim = output.shape[-1]\n",
    "            output = output[:, 1:].reshape(-1, output_dim)\n",
    "            tgt_flat = tgt[:, 1:].reshape(-1)\n",
    "            loss = criterion(output, tgt_flat)\n",
    "            total_loss += loss.item()\n",
    "            preds = output.argmax(dim=1)\n",
    "            non_pad_mask = tgt_flat != out_vocab['<pad>']\n",
    "            correct = (preds == tgt_flat) & non_pad_mask\n",
    "            total_correct += correct.sum().item()\n",
    "            total_tokens += non_pad_mask.sum().item()\n",
    "        for src, tgt in sampled_loader:\n",
    "            src, tgt = src.to(device), tgt.to(device)\n",
    "            for i in range(src.shape[0]):\n",
    "                src_sent = src[i].unsqueeze(0)\n",
    "                tgt_sent = tgt[i].cpu().numpy()\n",
    "                pred_sent = beam_search(model, src_sent, in_vocab, out_vocab, max_len, device, beam_size=3)\n",
    "                ref_sent = [idx2word.get(idx, '<unk>') for idx in tgt_sent if idx not in [out_vocab['<pad>'], out_vocab['<sos>'], out_vocab['<eos>']]]\n",
    "                bleu = sentence_bleu([ref_sent], pred_sent, smoothing_function=smoothie)\n",
    "                bleu_scores.append(bleu)\n",
    "    avg_loss = total_loss / len(iterator)\n",
    "    accuracy = total_correct / total_tokens if total_tokens > 0 else 0\n",
    "    avg_bleu = sum(bleu_scores) / len(bleu_scores) if bleu_scores else 0\n",
    "    return avg_loss, accuracy, avg_bleu\n",
    "\n",
    "# Hàm huấn luyện\n",
    "def train(model, train_loader, val_loader, optimizer, criterion, in_vocab, out_vocab, max_len, device, num_epochs=15, clip=1, patience=3):\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    val_accuracies = []\n",
    "    val_bleu_scores = []\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    log_file = \"/kaggle/working/results/training_log.txt\"\n",
    "    with open(log_file, 'w', encoding='utf-8') as f:\n",
    "        f.write(\"Epoch,Train Loss,Val Loss,Val Accuracy,Val BLEU,VRAM (MB),Epoch Time (s)\\n\")\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        batch_count = 0\n",
    "        start_time = time.time()\n",
    "        for src, tgt in train_loader:\n",
    "            src, tgt = src.to(device, non_blocking=True), tgt.to(device, non_blocking=True)\n",
    "            optimizer.zero_grad()\n",
    "            with torch.amp.autocast('cuda'):\n",
    "                output = model(src, tgt, teacher_forcing_ratio=0.8)\n",
    "                output_dim = output.shape[-1]\n",
    "                output = output[:, 1:].reshape(-1, output_dim)\n",
    "                tgt = tgt[:, 1:].reshape(-1)\n",
    "                loss = criterion(output, tgt)\n",
    "            scaler.scale(loss).backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            epoch_loss += loss.item()\n",
    "            batch_count += 1\n",
    "        train_loss = epoch_loss / batch_count\n",
    "        val_loss, val_accuracy, val_bleu = calculate_metrics(model, val_loader, in_vocab, out_vocab, device, max_len, criterion)\n",
    "        epoch_time = time.time() - start_time\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        val_accuracies.append(val_accuracy)\n",
    "        val_bleu_scores.append(val_bleu)\n",
    "        scheduler.step(val_loss)\n",
    "        with open(log_file, 'a', encoding='utf-8') as f:\n",
    "            f.write(f\"{epoch+1},{train_loss:.3f},{val_loss:.3f},{val_accuracy:.3f},{val_bleu:.3f},{torch.cuda.memory_allocated()/1024**2:.2f},{epoch_time:.2f}\\n\")\n",
    "        print(f'Epoch: {epoch+1:02}')\n",
    "        print(f'\\tTrain Loss: {train_loss:.3f}')\n",
    "        print(f'\\tVal Loss: {val_loss:.3f}')\n",
    "        print(f'\\tVal Accuracy: {val_accuracy:.3f}')\n",
    "        print(f'\\tVal BLEU: {val_bleu:.3f}')\n",
    "        print(f'\\tEpoch Time: {epoch_time:.2f} seconds')\n",
    "        print(f'\\tVRAM allocated: {torch.cuda.memory_allocated()/1024**2:.2f} MB')\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            torch.save(model, '/kaggle/working/results/seq2seq_lstm_best.pt')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f'Early stopping triggered after epoch {epoch+1}')\n",
    "                break\n",
    "    return train_losses, val_losses, val_accuracies, val_bleu_scores\n",
    "\n",
    "# Hàm vẽ và lưu biểu đồ\n",
    "def plot_and_save_metrics(train_losses, val_losses, val_accuracies, val_bleu_scores):\n",
    "    epochs = range(1, len(train_losses) + 1)\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(epochs, train_losses, label='Train Loss')\n",
    "    plt.plot(epochs, val_losses, label='Validation Loss')\n",
    "    plt.title('Loss over Epochs')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.savefig('/kaggle/working/results/loss_plot.png')\n",
    "    plt.close()\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(epochs, val_accuracies, label='Validation Accuracy', color='green')\n",
    "    plt.title('Validation Accuracy over Epochs')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.savefig('/kaggle/working/results/accuracy_plot.png')\n",
    "    plt.close()\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(epochs, val_bleu_scores, label='Validation BLEU', color='blue')\n",
    "    plt.title('Validation BLEU Score over Epochs')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('BLEU Score')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.savefig('/kaggle/working/results/bleu_plot.png')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-31T11:09:34.870265Z",
     "iopub.status.busy": "2025-05-31T11:09:34.869393Z",
     "iopub.status.idle": "2025-05-31T19:36:49.463142Z",
     "shell.execute_reply": "2025-05-31T19:36:49.461764Z",
     "shell.execute_reply.started": "2025-05-31T11:09:34.870240Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01\n",
      "\tTrain Loss: 1.806\n",
      "\tVal Loss: 0.304\n",
      "\tVal Accuracy: 0.910\n",
      "\tVal BLEU: 0.835\n",
      "\tEpoch Time: 15007.65 seconds\n",
      "\tVRAM allocated: 218.56 MB\n",
      "Epoch: 02\n",
      "\tTrain Loss: 0.278\n",
      "\tVal Loss: 0.270\n",
      "\tVal Accuracy: 0.927\n",
      "\tVal BLEU: 0.872\n",
      "\tEpoch Time: 15028.77 seconds\n",
      "\tVRAM allocated: 218.56 MB\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_35/1331046030.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Kiểm tra trên test set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mtest_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_accuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_bleu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_vocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout_vocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'\\nTest Loss: {test_loss:.3f}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Test Accuracy: {test_accuracy:.3f}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_35/1462828390.py\u001b[0m in \u001b[0;36mcalculate_metrics\u001b[0;34m(model, iterator, in_vocab, out_vocab, device, max_len, criterion)\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0mtgt_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtgt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_flat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m             \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m             \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m             \u001b[0mnon_pad_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtgt_flat\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mout_vocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'<pad>'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Huấn luyện\n",
    "num_epochs = 2\n",
    "max_len = 50\n",
    "train_losses, val_losses, val_accuracies, val_bleu_scores = train(\n",
    "    model, train_loader, val_loader, optimizer, criterion,\n",
    "    train_dataset.in_vocab, train_dataset.out_vocab, max_len, device, num_epochs=num_epochs, patience=3\n",
    ")\n",
    "\n",
    "# Vẽ và lưu biểu đồ\n",
    "plot_and_save_metrics(train_losses, val_losses, val_accuracies, val_bleu_scores)\n",
    "\n",
    "# Kiểm tra trên test set\n",
    "test_loss, test_accuracy, test_bleu = calculate_metrics(model, test_loader, train_dataset.in_vocab, train_dataset.out_vocab, device, max_len, criterion)\n",
    "print(f'\\nTest Loss: {test_loss:.3f}')\n",
    "print(f'Test Accuracy: {test_accuracy:.3f}')\n",
    "print(f'Test BLEU: {test_bleu:.3f}')\n",
    "\n",
    "# Test suy luận trên test set (giữ mã hóa)\n",
    "for i in range(5):\n",
    "    src, tgt = test_dataset[i]\n",
    "    src = src.unsqueeze(0)\n",
    "    pred_tokens = beam_search(model, src, train_dataset.in_vocab, train_dataset.out_vocab, max_len, device, beam_size=3)\n",
    "    src_words = [train_dataset.in_vocab.get(k, '<unk>') for k in src[0].cpu().numpy() if k != train_dataset.in_vocab['<pad>']]\n",
    "    tgt_words = [train_dataset.out_vocab.get(k, '<unk>') for k in tgt.cpu().numpy() if k not in [train_dataset.out_vocab['<pad>'], train_dataset.out_vocab['<sos>'], train_dataset.out_vocab['<eos>']]]\n",
    "    print(f\"Source: {' '.join(src_words)}\")\n",
    "    print(f\"Target (normalized): {' '.join(tgt_words)}\")\n",
    "    print(f\"Predicted (normalized): {' '.join(pred_tokens)}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7545831,
     "sourceId": 11996158,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
